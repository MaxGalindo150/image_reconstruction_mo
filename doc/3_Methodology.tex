\section{Simulation and Experimental Setup} \label{sec:method}

The experiments conducted in this study are based on synthetic data, allowing precise control over the noise levels and the number of samples. This approach provides valuable insights into the accuracy and stability of the proposed methods for estimating \( \mathbf{d} \) and the absorption profile \( \mathbf{\mu} \). The simulations were generated using the Linear State Space Model described in Section \ref{sec:lit:two} and the forward model outlined in Section \ref{sec:lit:one}, which relies on the photoacoustic effectâ€”a phenomenon where light absorption by a material generates sound waves.

As discussed in Section \ref{sec:lit:two}, the linear state space model allows the estimation of the absorption profile \( \mathbf{\mu} \) by solving the following linear matrix equation:

\begin{equation} \label{eq:linear}
    \mathbf{y} = \mathbf{H} \mathbf{d} + \mathbf{w},
\end{equation}

where \( \mathbf{y} \) represents the measurements, \( \mathbf{H} \) is the system matrix derived from experimental data, \( \mathbf{d} \) is the data vector, and \( \mathbf{w} \) is the noise term with distribution \( \mathcal{N}(0, \sigma^2) \). From the linear state space model, \( \mathbf{\mu} \) can be computed using the relationship:

\begin{equation}
    \mathbf{d} = \begin{bmatrix}
                    d_1 \\
                    d_2 \\
                    d_3 \\
                    \vdots \\
                    d_{N_z}    
                \end{bmatrix}
             =
                \begin{bmatrix}
                    \mu_0 \\
                    \mu_1 a_0 \\
                    \mu_2 a_1 a_0 \\
                    \vdots \\
                    \mu_{N_z -1} a_{N_z - 2} a_{N_z - 3} \cdots a_1 a_0
                \end{bmatrix}.
\end{equation}

The equation \ref{eq:linear} can theoretically be solved for \( \mathbf{d} \) using least squares regression. However, due to the ill-conditioned nature of the problem, this approach often results in unstable solutions. To stabilize the solution, regularization techniques are applied, leading to the following optimization problem:

\begin{equation} \label{eq:regularization:one}
    \hat{\mathbf{d}} = \argmin_{\mathbf{d}} \left\{ \left\| \mathbf{y} - \mathbf{H} \mathbf{d} \right\|_2^2 + \lambda \left\| \mathbf{d} \right\|_2^2 \right\},
\end{equation}

where \( \lambda \) is the regularization parameter that controls the trade-off between the data fidelity term and the regularization term. The solution to this problem is given by:

\begin{equation}
    \hat{\mathbf{d}} = \left( \mathbf{H}^T \mathbf{H} + \lambda \mathbf{I} \right)^{-1} \mathbf{H}^T \mathbf{y}.
\end{equation}

\section{Tikhonov Regularization for Image Reconstruction} \label{sec:method:first}

Tikhonov regularization is a widely used technique for stabilizing ill-conditioned problems. By introducing a regularization term that penalizes large coefficients, it prevents overfitting and enhances the stability of the solution. In the context of image reconstruction, this method solves equation \ref{eq:regularization:one} by balancing data fidelity and regularization.

The choice of the regularization parameter \( \lambda \) is crucial. The L-curve method, which plots the residual norm \( \left\| \mathbf{y} - \mathbf{H} \hat{\mathbf{d}} \right\|_2 \) against the regularization norm \( \left\| \hat{\mathbf{d}} \right\|_2 \), helps identify the optimal \( \lambda \). The corner of the L-curve represents the best trade-off between accuracy and stability.

However, Tikhonov regularization typically provides a single solution, which may not fully capture the range of possible trade-offs between objectives. This limitation motivates the use of multi-objective optimization.

\section{Multi-Objective Optimization for Image Reconstruction} \label{sec:method:second}

Multi-objective optimization enhances the reconstruction process by providing a diverse set of solutions, each capturing a unique trade-off between competing objectives. In the context of ill-posed regularization problems, these objectives are typically the accuracy of the data fit, quantified by \( \left\| \mathbf{y} - \mathbf{H} \mathbf{d} \right\|_2 \), and the stability of the solution, measured by \( \left\| \mathbf{d} \right\|_2 \).

We use NSGA-II to estimate the data vector \( \mathbf{d} \) and subsequently derive the absorption profile \( \mathbf{\mu} \). The algorithm generates a Pareto front of solutions, offering a diverse set of trade-offs between accuracy and stability. Unlike traditional approaches that yield a single solution, the Pareto front allows users to select the solution that best aligns with specific application requirements or constraints.

The choice of objectives is inspired by the L-curve method, where the optimal regularization parameter \( \lambda \) balances data fidelity and regularization. Minimizing \( \left\| \mathbf{d} \right\|_2 \) aligns with Tikhonov regularization, ensuring solution stability while mitigating numerical instabilities and overfitting. Each solution in the Pareto front corresponds to a unique balance between these objectives, enabling a comprehensive exploration of the trade-offs involved.

\section{Hybrid Optimization for Image Reconstruction} \label{sec:method:third}

The hybrid optimization approach combines Tikhonov regularization and NSGA-II to leverage the strengths of both techniques. Tikhonov regularization provides an initial estimate of \( \mathbf{d} \), offering a stable starting point for the optimization process. NSGA-II then refines this estimate, exploring a broader solution space and generating a diverse set of solutions in the Pareto front.

By using the Tikhonov solution as the initial population for NSGA-II, the algorithm achieves faster convergence and produces a more diverse Pareto front. This approach not only improves the quality of the reconstruction but also enables the selection of solutions tailored to specific trade-offs between accuracy and stability. 

The Pareto front generated by this hybrid method offers a comprehensive range of solutions. Users can evaluate and choose the most suitable solution based on application-specific requirements, such as prioritizing higher accuracy or enhanced stability. This flexibility represents a significant advantage over single-solution methods, which may not adequately address the diverse needs of real-world applications.
